{
  "name": "localai",
  "slug": "localai",
  "categories": [
    13
  ],
  "date_created": "2025-11-23",
  "interface_port": null,
  "documentation": "github.com/mudler/LocalAI",
  "website": "localai.io",
  "source_code": "github.com/mudler/LocalAI",
  "logo": "https://cdn.jsdelivr.net/gh/selfhst/icons/webp/localai.webp",
  "description": "LocalAI is a self-hosted artificial intelligence inference engine that runs LLMs locally with no external API calls. It supports model selection, GPU acceleration, and provides an API compatible with OpenAI.",
  "install_methods": [
    {
      "platform": {
        "desktop": {
          "linux": false,
          "windows": false,
          "macos": false
        },
        "mobile": {
          "android": false,
          "ios": false
        },
        "web_app": false,
        "browser_extension": false,
        "cli_only": false,
        "hosting": {
          "self_hosted": false,
          "saas": false,
          "managed_cloud": false
        },
        "ui": {
          "cli": false,
          "gui": false,
          "web_ui": false,
          "api": false,
          "tui": false
        }
      }
    }
  ],
  "default_credentials": {
    "username": null,
    "password": null
  },
  "notes": [],
  "platform": {
    "desktop": false,
    "mobile": false,
    "web_extensions": false,
    "hosting": false,
    "ui_interface": true
  },
  "deployment": {
    "script": true,
    "docker": true,
    "docker_compose": true,
    "helm": true,
    "kubernetes": true,
    "terraform": true,
    "paths": {
      "script": "/public/manifest/dummy/script.sh",
      "docker": "/public/manifest/dummy/Dockerfile",
      "docker_compose": "/public/manifest/dummy/docker-compose.yaml",
      "helm": "/public/manifest/dummy/helm.yaml",
      "kubernetes": "/public/manifest/dummy/k8s-deployment.yaml",
      "terraform": "/public/manifest/dummy/main.tf"
    }
  }
}